{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "consecutive-confidence",
   "metadata": {},
   "source": [
    "# Prep\n",
    "\n",
    "Setting up some prior functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rational-neighborhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126 True\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-interim",
   "metadata": {},
   "source": [
    "# Load a model\n",
    "\n",
    "First we have to decide if our model should be pretrained. \n",
    "\n",
    "This greatly depends on the size of a dataset. Smaller datasets rely more on finetuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "printable-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = False\n",
    "\n",
    "if pretrained:\n",
    "    # Get pretrained weights\n",
    "    checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                url='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth',\n",
    "                map_location='cpu',\n",
    "                check_hash=True)\n",
    "    # Remove class weights\n",
    "    del checkpoint[\"model\"][\"class_embed.weight\"]\n",
    "    del checkpoint[\"model\"][\"class_embed.bias\"]\n",
    "\n",
    "    # SaveOGH\n",
    "    torch.save(checkpoint,\n",
    "               'detr-r50_no-class-head.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-asthma",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Our dataset should be loadable as a COCO format\n",
    "\n",
    "This allows us to use the pycocotools to load the data dict for the main python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "laden-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = \"coco\" # alternatively, implement your own coco-type dataset loader in datasets and add this \"key\" to datasets/__init__.py\n",
    "\n",
    "dataDir='datasets/sentinel2_coco' \n",
    "# dataDir='datasets/ship_playground_coco' \n",
    "num_classes = 2 # this int should be the actual number of classes + 1 (for no class)\n",
    "\n",
    "outDir = 'outputs'\n",
    "resume = \"detr-r50_no-class-head.pth\" if pretrained else \"scratch\"\n",
    "frozen_weights = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-version",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We use the main.py script to run our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hazardous-possibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python main.py \\\n",
    "  --dataset_file $dataset_file \\\n",
    "  --coco_path $dataDir \\\n",
    "  --output_dir $outDir \\\n",
    "  --resume $resume \\\n",
    "  --num_classes $num_classes \\\n",
    "  --lr 1e-4 \\\n",
    "  --lr_backbone 1e-5 \\\n",
    "  --epochs 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-sister",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Quick and easy overview of the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "together-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.plot_utils import plot_logs\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "log_directory = [Path(outDir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tropical-bulgarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> missing log.txt.  Have you gotten to Epoch 1 in training?\n",
      "--> full path of missing log file: outputs\\log.txt\n"
     ]
    }
   ],
   "source": [
    "fields_of_interest = (\n",
    "    'loss',\n",
    "    'mAP',\n",
    "    )\n",
    "\n",
    "plot_logs(log_directory,\n",
    "          fields_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "composed-characterization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> missing log.txt.  Have you gotten to Epoch 1 in training?\n",
      "--> full path of missing log file: outputs\\log.txt\n"
     ]
    }
   ],
   "source": [
    "fields_of_interest = (\n",
    "    'loss_ce',\n",
    "    'loss_bbox',\n",
    "    'loss_giou',\n",
    "    )\n",
    "\n",
    "plot_logs(log_directory,\n",
    "          fields_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "healthy-tournament",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> missing log.txt.  Have you gotten to Epoch 1 in training?\n",
      "--> full path of missing log file: outputs\\log.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "fields_of_interest = (\n",
    "    'class_error',\n",
    "    'cardinality_error_unscaled',\n",
    "    )\n",
    "\n",
    "plot_logs(log_directory,\n",
    "          fields_of_interest)   \n",
    "\n",
    "# from util.plot_utils import plot_precision_recall\n",
    "# plot_precision_recall(log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f25148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
